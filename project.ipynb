{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d6375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4e089cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62968f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prasad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import networkx as nx\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "splitter = SentenceSplitter(language='en')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca5580c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "s_tokenizer = AutoTokenizer.from_pretrained(s_model_name)\n",
    "s_model = AutoModel.from_pretrained(s_model_name)\n",
    "s_model.eval()  # set model to evaluation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa7bfd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd9728cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prasad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Prasad\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "B_model_name = \"facebook/bart-large-cnn\"\n",
    "B_tokenizer = BartTokenizer.from_pretrained(B_model_name)\n",
    "B_model = BartForConditionalGeneration.from_pretrained(B_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44da57a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_resource\n",
    "def scibert_generated_summary(input_text):\n",
    "\n",
    "    #scibert summarization\n",
    "    # Step 1: Sentence Segmentation\n",
    "    sentences = sent_tokenize(input_text)\n",
    "     \n",
    "    # Step 2: Encode each sentence using SciBERT\n",
    "    def encode_text(text):\n",
    "        \"\"\"Encodes text and returns the [CLS] token embedding.\"\"\"\n",
    "        inputs = s_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "           outputs = s_model(**inputs)\n",
    "        # Using the [CLS] token embedding as a representation (first token)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "        return cls_embedding\n",
    "    \n",
    "    # Encode all sentences\n",
    "    sentence_embeddings = [encode_text(sent) for sent in sentences]\n",
    "\n",
    "    # Step 3: Compute a simple document embedding by averaging sentence embeddings\n",
    "    document_embedding = np.mean(sentence_embeddings, axis=0, keepdims=True)\n",
    "\n",
    "    # Step 4: Compute cosine similarity of each sentence with the document embedding\n",
    "    # Reshape each sentence embedding to 2D for cosine_similarity computation\n",
    "    similarity_scores = [\n",
    "        cosine_similarity(embedding.reshape(1, -1), document_embedding)[0][0]\n",
    "        for embedding in sentence_embeddings\n",
    "    ]\n",
    "    \n",
    "    # For example, select the top 3 sentences.\n",
    "    k = 10\n",
    "    \n",
    "    top_k_indices = np.argsort(similarity_scores)[-k:][::-1]\n",
    "\n",
    "    top_k_indices_sorted = sorted(top_k_indices)\n",
    "    scibert_summary = \"\\n\".join([sentences[i] for i in top_k_indices_sorted])\n",
    "\n",
    "    return scibert_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3a7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_resource\n",
    "def graph_generatred_summary(input_text,num_sentences=10):\n",
    "\n",
    "    sentences = nltk.sent_tokenize(input_text)\n",
    "    \n",
    "    # Step 3: Compute TF-IDF and cosine similarity\n",
    "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
    "    similarity_matrix = cosine_similarity(vectorizer)\n",
    "    \n",
    "    # Step 4: Build graph and apply PageRank\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(graph)\n",
    "    \n",
    "    # Step 5: Rank sentences and pick top N\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    graph_summary = \" \".join([sentence for _, sentence in ranked_sentences[:num_sentences]])\n",
    "    \n",
    "    return graph_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09da52a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_resource\n",
    "def bart_summary(sentence3, max_length=400, min_length=50):\n",
    "    \"\"\"Generate an abstractive summary using BART\"\"\"\n",
    "    # Tokenize input text\n",
    "    inputs = B_tokenizer(sentence3, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = B_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        length_penalty=1.5,\n",
    "        num_beams=6,\n",
    "        no_repeat_ngram_size=3,  \n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    # Decode and return the summary\n",
    "    return B_tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb1d04fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_resource\n",
    "def get_response(input_text,num_return_sequences):\n",
    "      batch = tokenizer.prepare_seq2seq_batch([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "      translated = model.generate(**batch,max_length=60,num_beams=10, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "      tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "      return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62a47e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Prasad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050114e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
